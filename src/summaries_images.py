summaries = {
    "extracted_image_1.png": "Figure Summary (extracted_image_1.png): This is a schematic diagram of the Transformer architecture used in deep learning for sequence modeling. It shows the encoder-decoder structure, where the encoder consists of stacked layers of multi-head self-attention followed by feed-forward networks, each with layer normalization and residual connections. The decoder mirrors this but includes an additional masked self-attention block to ensure autoregressive generation. Positional encoding is added to the input and output embeddings to retain sequence order, and the final output probabilities are generated through a linear layer followed by softmax.",
    "extracted_image_2.png": "Figure Summary (extracted_image_2.png): This diagram illustrates the scaled dot-product attention mechanism, a key component of Transformer models. It shows how the query (Q), key (K), and value (V) vectors are processed. Q and K undergo a matrix multiplication to compute attention scores, which are optionally masked and then scaled. The scores are passed through a softmax layer to produce attention weights, which are then used to weight the value vectors (V) via another matrix multiplication, yielding the final attention output.",
    "extracted_image_3.png": "Figure Summary (extracted_image_3.png): This diagram explains the Multi-Head Attention mechanism used in Transformer models. It shows how input vectors for queries (Q), keys (K), and values (V) are first linearly projected into multiple subspaces (heads). Each head performs scaled dot-product attention independently. The results of all heads are then concatenated and passed through another linear layer to produce the final output. This setup allows the model to jointly attend to information from different representation subspaces at different positions.",
    "extracted_image_4.png": "Figure Summary (extracted_image_4.png): This is a visualization of attention weights in a Transformer decoder layer during text generation. The horizontal axis shows the sequence of previously generated tokens, while the vertical axis shows the current decoding step. Colored lines and squares indicate which previous tokens the model attends to when predicting the next word. Stronger attention (darker or thicker lines) shows greater influence on the prediction. This particular example demonstrates how the model focuses on relevant context such as 'making', 'registration', and 'process' while generating the phrase 'more difficult'.",
    "extracted_image_5.png": "Figure Summary (extracted_image_4.png): This is a visualization of attention weights in a Transformer decoder layer during text generation. The horizontal axis shows the sequence of previously generated tokens, while the vertical axis shows the current decoding step. Colored lines and squares indicate which previous tokens the model attends to when predicting the next word. Stronger attention (darker or thicker lines) shows greater influence on the prediction. This particular example demonstrates how the model focuses on relevant context such as 'making', 'registration', and 'process' while generating the phrase 'more difficult'.",
    "extracted_image_6.png": "Figure Summary (extracted_image_4.png): This is a visualization of attention weights in a Transformer decoder layer during text generation. The horizontal axis shows the sequence of previously generated tokens, while the vertical axis shows the current decoding step. Colored lines and squares indicate which previous tokens the model attends to when predicting the next word. Stronger attention (darker or thicker lines) shows greater influence on the prediction. This particular example demonstrates how the model focuses on relevant context such as 'making', 'registration', and 'process' while generating the phrase 'more difficult'.",
}
